# -*- coding: utf-8 -*-
"""nfsw_detection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1OH7G-x_p74sfFDvlbL9sj3KBJJxQMYKq

**NFSW Detections for Study Materials/Contents**
"""

# pip install transformers torchvision torch pillow

# pip install transformers torch pdfplumber python-docx

"""**content's file: .docx .txt .pdf**"""

# nsfw_detection.py
import sys
import os
import requests
from io import BytesIO
import pdfplumber
from docx import Document
from transformers import pipeline
from transformers import AutoProcessor, AutoModelForImageClassification
from PIL import Image
import torch

# Text toxicity classifier (load once)
classifier = pipeline("text-classification", model="unitary/toxic-bert", top_k=None)

def download_file(url):
    response = requests.get(url)
    response.raise_for_status()
    return response.content

def extract_text_from_bytes(file_bytes, ext):
    if ext == ".pdf":
        with pdfplumber.open(BytesIO(file_bytes)) as pdf:
            return "\n".join(page.extract_text() or "" for page in pdf.pages)
    elif ext == ".docx":
        doc = Document(BytesIO(file_bytes))
        return "\n".join([para.text for para in doc.paragraphs])
    elif ext == ".txt":
        return file_bytes.decode("utf-8", errors="ignore")
    else:
        raise ValueError("Unsupported file type. Use .pdf, .docx, or .txt")

def is_nsfw_text(text):
    results = classifier(text[:512])
    labels = results[0]
    toxic_score = sum([item["score"] for item in labels if item["label"] in ["toxicity", "obscene", "sexual_explicit", "threat"]])
    return toxic_score > 0.5

def is_nsfw_image(image_bytes):
    model_name = "Falconsai/nsfw_image_detection"
    processor = AutoProcessor.from_pretrained(model_name)
    model = AutoModelForImageClassification.from_pretrained(model_name)
    image = Image.open(BytesIO(image_bytes)).convert("RGB")
    inputs = processor(images=image, return_tensors="pt")
    with torch.no_grad():
        outputs = model(**inputs)
        logits = outputs.logits
        predicted_class = logits.argmax(-1).item()
    labels = model.config.id2label
    predicted_label = labels[predicted_class].lower()
    nsfw_labels = ["porn", "nude", "nfsw", "explicit", "tits", "hentai", "vulgar", "sexy"]
    return predicted_label in nsfw_labels

def main():
    if len(sys.argv) != 3:
        print("Usage: python nsfw_detection.py <file_url> <file_type>")
        sys.exit(1)

    file_url = sys.argv[1]
    file_type = sys.argv[2].lower()

    try:
        file_bytes = download_file(file_url)

        if file_type == "text":
            # Infer file extension from URL for extractor (fallback to .txt)
            ext = os.path.splitext(file_url)[1].lower()
            if ext not in [".pdf", ".docx", ".txt"]:
                ext = ".txt"

            text_content = extract_text_from_bytes(file_bytes, ext)
            if is_nsfw_text(text_content):
                print("âš ï¸â˜ ï¸ğŸš¨ Explicit/NSFW content detected âš ï¸â˜ ï¸ğŸš¨.")
            else:
                print("âœ…ğŸ‘Œ Content appears safe âœ…ğŸ¤.")

        elif file_type == "image":
            if is_nsfw_image(file_bytes):
                print("âš ï¸â˜ ï¸ğŸš¨ Explicit/NSFW content detected âš ï¸â˜ ï¸ğŸš¨.")
            else:
                print("âœ…ğŸ‘Œ Content appears safe âœ…ğŸ¤.")

        else:
            print("Unsupported file_type argument. Use 'text' or 'image'.")
            sys.exit(1)

    except Exception as e:
        print(f"Error during NSFW detection: {str(e)}")
        sys.exit(1)

if __name__ == "__main__":
    main()
